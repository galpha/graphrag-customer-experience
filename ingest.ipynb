{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Loose Entity Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\r\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting pypdf\r\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\r\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/zachblumenfeld/interpeters/graphrag-support-example/lib/python3.9/site-packages (from pypdf) (4.11.0)\r\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m290.4/290.4 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: python-dotenv, pypdf\r\n",
      "Successfully installed pypdf-4.2.0 python-dotenv-1.0.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv pypdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('exp.env', override=True)\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "#OPENAI\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"attention-is-all-you-need.pdf\")\n",
    "pages = loader.load_and_split()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence', metadata={'source': 'attention-is-all-you-need.pdf', 'page': 1})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "extract_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert researcher that needs to extract relevant facts from the below paper excerpt \"\n",
    "            \"for use in a source of truth knowledge base. \"\n",
    "            \"The knowledge base will be used to help non-experts answer critical questions about the research materials.\"\n",
    "            \"The facts should be extracted in the form of a a knowledge triple: subject, predicate object.\"\n",
    "            \"each subject, predicate, and object should only be 1 to a few words. Avoid sentences and conjunctions, \"\n",
    "            \"instead you can create more facts. \"\n",
    "            \"Avoid object containing a preposition or adverb, instead create additional facts\"\n",
    "            \"Given that this is a source of truth, please ensure the facts come only from the provided text, \"\n",
    "            \"Do not create fictitious data or impute missing values.\"\n",
    "            \"Only include facts that are clear, non-ambiguous, and relevant to the research\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    \"\"\"A useful fact\"\"\"\n",
    "    subject: str = Field(description=\"Subject: Entity being described\")\n",
    "    predicate: str = Field(description=\"Predicate: The property or action of the subject that is being described\")\n",
    "    object: str = Field(description=\"Object: The value of the property or action being described\")\n",
    "\n",
    "class Facts(BaseModel):\n",
    "    \"\"\"A series of useful facts\"\"\"\n",
    "    facts: List[Fact]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_extractor=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "chain_extractor = extract_prompt | llm_extractor.with_structured_output(Facts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "facts = chain_extractor.invoke(pages[1].page_content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Transformer) - [does not use] -> (convolution)\n",
      "(Transformer) - [model architecture] -> (eschewing recurrence)\n",
      "(Transformer) - [computes representations of] -> (input and output)\n",
      "(Recurrent neural networks) - [established as] -> (state of the art approaches)\n",
      "(Recent work) - [improved] -> (model performance)\n",
      "(Transformer) - [does not use] -> (sequence-aligned RNNs)\n",
      "(Recurrent models) - [generate] -> (sequence of hidden states)\n",
      "(Transformer) - [reduces] -> (number of operations between positions)\n",
      "(ConvS2S) - [uses] -> (convolutional neural networks)\n",
      "(Attention mechanisms) - [integral part of] -> (sequence modeling)\n",
      "(ByteNet) - [uses] -> (convolutional neural networks)\n",
      "(Transformer) - [relying on] -> (attention mechanism)\n",
      "(End-to-end memory networks) - [based on] -> (recurrent attention mechanism)\n",
      "(Long short-term memory) - [established as] -> (state of the art approach)\n",
      "(Gated recurrent neural networks) - [established as] -> (state of the art approaches)\n",
      "(Recurrent neural networks) - [used in] -> (sequence modeling)\n",
      "(Transformer) - [allows for] -> (more parallelization)\n",
      "(Transformer) - [trained on] -> (eight P100 GPUs)\n",
      "(Recurrent models) - [preclude] -> (parallelization within training examples)\n",
      "(Transformer) - [counteracts] -> (reduced effective resolution)\n",
      "(Transformer) - [motivates] -> (self-attention)\n",
      "(Transformer) - [first transduction model relying on] -> (self-attention)\n",
      "(Transformer) - [discusses advantages over] -> (models)\n",
      "(Transformer) - [can reach] -> (new state of the art in translation quality)\n",
      "(Recurrent neural networks) - [used in] -> (transduction problems)\n",
      "(Recurrent models) - [factor computation along] -> (symbol positions)\n",
      "(Recent work) - [achieved] -> (significant improvements in computational efficiency)\n",
      "(Attention mechanisms) - [integral part of] -> (transduction models)\n",
      "(Self-attention) - [used in] -> (variety of tasks)\n",
      "(Extended Neural GPU) - [uses] -> (convolutional neural networks)\n"
     ]
    }
   ],
   "source": [
    "def stringify_facts(f:Facts):\n",
    "    res = set()\n",
    "    for fact in f.facts:\n",
    "        res.add(f'({fact.subject}) - [{fact.predicate}] -> ({fact.object})')\n",
    "    return res\n",
    "\n",
    "for s in stringify_facts(facts):\n",
    "    print(s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "relevance_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert researcher that needs to validate the relevance and clarity of extracted facts from a research paper\"\n",
    "            \"for use in a source of truth knowledge base. \"\n",
    "            \"The knowledge base will be used to help non-experts answer critical questions about the research materials. \"\n",
    "            \"To be relevant The fact should make sense, be non-ambiguous, and relevant to research on their own. \"\n",
    "            \"for example the fact (Recent work) - [improving] -> (model performance) is not relevant since the 'work' and 'model' are ambiguous, \"\n",
    "            \"however the fact (Transformer) - [rely on] -> (attention mechanism) is relevant since \"\n",
    "            \"'Transformer' and 'attention mechanism' are colloquially understood to be specific model types and components in the research.\"\n",
    "            \"In your response only return relevant facts according to this criteria.\"\n",
    "            \"Do not create fictitious data or impute missing values. \"\n",
    "            \"Do not alter any of the facts. \"\n",
    "            \"Only include facts from the ones provided.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "chain_filterer = (relevance_prompt | llm_extractor.with_structured_output(Facts)).with_types(input_type=Facts, output_type=Facts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "filtered_facts = chain_filterer.invoke(facts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Facts: 30\n",
      "Relevant Facts: 28\n"
     ]
    }
   ],
   "source": [
    "print(f'Extracted Facts: {len(facts.facts)}')\n",
    "print(f'Relevant Facts: {len(filtered_facts.facts)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped facts:\n",
      "(Recent work) - [achieved] -> (significant improvements in computational efficiency)\n",
      "(Recent work) - [improved] -> (model performance)\n"
     ]
    }
   ],
   "source": [
    "facts_set = stringify_facts(facts)\n",
    "filtered_facts_set = stringify_facts(filtered_facts)\n",
    "print('Dropped facts:')\n",
    "for fact in facts_set.difference(filtered_facts_set):\n",
    "    print(fact)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Classes and Functions for Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class Chunk(BaseModel):\n",
    "    \"\"\"A chunk of a source from which facts were extracted\"\"\"\n",
    "    id:str\n",
    "    sourceId:str\n",
    "    seqId:int\n",
    "    text:str\n",
    "\n",
    "\n",
    "class Source(BaseModel):\n",
    "    \"\"\"A source of facts\"\"\"\n",
    "    id:str\n",
    "    name:str\n",
    "    type:str\n",
    "    url:str\n",
    "\n",
    "class CitedFact(BaseModel):\n",
    "    \"\"\"A useful fact with a source chunk id\"\"\"\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    chunkId: str\n",
    "    def __init__(self, fact: Fact, chunk:Chunk):\n",
    "        super().__init__(subject = fact.subject.lower(),\n",
    "                         predicate = fact.predicate.lower(),\n",
    "                         object = fact.object.lower(),\n",
    "                         chunkId = chunk.id)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import Tuple\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def insert_sources(source:Source, chunks: List[Chunk]):\n",
    "    #source node\n",
    "    graph.query('''\n",
    "    MERGE(s:Source {id:$id})\n",
    "    SET s.name = $name, s.type=$type, s.url=$url\n",
    "    ''', params={source.dict})\n",
    "\n",
    "    #chunk nodes and rels to source\n",
    "    graph.query('''\n",
    "    UNWIND $chunks AS chunk\n",
    "    MATCH(s:Source {id:chunk.sourceId})\n",
    "    MERGE(c:Chunk {id:chunk.id})\n",
    "    SET c.seqId=chunk.seqId, c.text=chunk.text\n",
    "    MERGE (c)-[:PART_OF]->(s)\n",
    "    ''', params={'chunks': [chunk.dict() for chunk in chunks]})\n",
    "\n",
    "merge_entity_query = \"\"\"\n",
    "    UNWIND $entities AS entity\n",
    "    MATCH(c:Chunk {id:entity.chunkId})\n",
    "    MERGE(e:Entity {id:entity.id})\n",
    "    MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "\"\"\"\n",
    "\n",
    "def insert_facts(facts: List[CitedFact]):\n",
    "    #insert subjects\n",
    "    subjects = [{'id':fact.subject, 'chunkId':fact.chunkId} for fact in facts]\n",
    "    graph.query(merge_entity_query, params={'entities': subjects})\n",
    "\n",
    "    #insert objects\n",
    "    objects = [{'id':fact.object, 'chunkId':fact.chunkId} for fact in facts]\n",
    "    graph.query(merge_entity_query, params={'entities': objects})\n",
    "\n",
    "    #insert predicates\n",
    "    graph.query(\"\"\"\n",
    "    UNWIND $facts AS fact\n",
    "    MATCH(s:Entity {id:fact.subject})\n",
    "    MATCH(o:Entity {id:fact.object})\n",
    "    MERGE (s)-[r:RELATES_TO {id:fact.predicate}]->(o)\n",
    "    ON CREATE SET r.chunkId = fact.chunkId\n",
    "    \"\"\", params={'facts': [fact.dict() for fact in facts]})\n",
    "\n",
    "\n",
    "def create_sources(docs: List[Document], name: str, doc_type: str, url: str) -> Tuple[Source, List[Chunk]]:\n",
    "    source = Source(id=str(uuid4()), name=name, type=doc_type, url=url)\n",
    "    chunks = []\n",
    "    for i in range(len(docs)):\n",
    "        chunks.append(Chunk(id=str(uuid4()), sourceId=source.id, seqId=i, text=docs[i].page_content))\n",
    "    return source, chunks\n",
    "\n",
    "\n",
    "def extract_facts(chunk: Chunk) -> List[CitedFact]:\n",
    "    unfiltered_facts = chain_extractor.invoke(chunk.text)\n",
    "    facts = chain_filterer.invoke(unfiltered_facts)\n",
    "    return [CitedFact(fact, chunk) for fact in facts]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "graph.query('CREATE CONSTRAINT entityId IF NOT EXISTS FOR (n:Entity) REQUIRE (n.id) IS UNIQUE;')\n",
    "graph.query('CREATE CONSTRAINT chunkId IF NOT EXISTS FOR (n:Chunk) REQUIRE (n.id) IS UNIQUE;')\n",
    "graph.query('CREATE CONSTRAINT sourceId IF NOT EXISTS FOR (n:Source) REQUIRE (n.id) IS UNIQUE;')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"formatting and ingesting sources\")\n",
    "source, chunks = create_sources(pages, \"Paper: Attention Is All You Need\", \"pdf\", \"https://arxiv.org/pdf/1706.03762\")\n",
    "insert_sources(source, chunks)\n",
    "\n",
    "cited_facts = []\n",
    "print(\"extracting facts\")\n",
    "for chunk in tqdm(chunks):\n",
    "    cited_facts.extend(extract_facts(chunk))\n",
    "print(\"ingesting facts\")\n",
    "insert_facts(cited_facts)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
